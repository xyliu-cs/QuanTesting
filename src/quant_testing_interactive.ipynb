{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T04:58:30.044075Z",
     "start_time": "2023-07-14T04:58:30.038082Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wv-AmxjjO7jB",
    "outputId": "b2bdc54a-d1ec-49b9-c0e1-15de6a92f8c6"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/huggingface/transformers -q\n",
    "# !pip install sentencepiece sacremoses -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:53:12.043457Z",
     "start_time": "2023-07-15T16:52:59.240674Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:53:20.395387Z",
     "start_time": "2023-07-15T16:53:13.990239Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnIjRLnMNMyY",
    "outputId": "653f7190-7f2d-46b0-b52d-62259d8a8fb0"
   },
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:53:21.393865Z",
     "start_time": "2023-07-15T16:53:21.386786Z"
    },
    "id": "hjSuDmJ_PYE5"
   },
   "outputs": [],
   "source": [
    "text = [\"Patrick O'Neill, the former chief creative officer at Theranos from 2014 to 2017, said he has removed much of his promotional work for the company from his professional website.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:53:26.024398Z",
     "start_time": "2023-07-15T16:53:22.655698Z"
    }
   },
   "outputs": [],
   "source": [
    "# translate Chinese to English\n",
    "tokenizer.src_lang = \"en_XX\"\n",
    "encoded_en = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# print(\"encoded tokens: \", encoded_en)\n",
    "generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"], max_new_tokens=200)\n",
    "generated_tokens_q = quantized_model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"], max_new_tokens=200)\n",
    "\n",
    "# move generated tokens back to CPU for further processing\n",
    "# generated_tokens = generated_tokens.cpu()  \n",
    "\n",
    "# print(\"generated tokens: \", generated_tokens)\n",
    "decoded_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "decoded_text_q = tokenizer.batch_decode(generated_tokens_q, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:53:27.993411Z",
     "start_time": "2023-07-15T16:53:27.986264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"帕特里克·奥尼尔(Patrick O'Neill)在2014年至2017年间担任Theranos首席创意总监,他说他已经从自己的专业网站中删除了该公司的宣传工作。\"]\n",
      "[\"帕特里克·奥尼尔(Patrick O'Neill)表示,他已从自己的专业网站中删除了该公司的宣传工作。\"]\n"
     ]
    }
   ],
   "source": [
    "print(decoded_text)\n",
    "print(decoded_text_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:54:32.309028Z",
     "start_time": "2023-07-15T16:54:32.294663Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "HF_API_URL = \"https://api-inference.huggingface.co/models/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "headers = {\"Authorization\": \"YOUR AUTHENTICATION CODE\"}\n",
    "\n",
    "def query(payload):\n",
    "    retries = 0\n",
    "    max_retries = 10\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.post(HF_API_URL, headers=headers, json=payload)\n",
    "            response.raise_for_status()  # This will raise an exception for HTTP errors\n",
    "            data = response.json()\n",
    "            # Check if the returned value is a list consisting of two numbers\n",
    "            if isinstance(data, list) and len(data) == 2 and all(isinstance(i, (int, float)) for i in data):\n",
    "                return data\n",
    "            else:\n",
    "                print(f'Returned value is not as expected, retrying... ({retries+1}/{max_retries})')\n",
    "        except (requests.exceptions.RequestException, requests.exceptions.ConnectionError, ValueError) as e:\n",
    "            print(f'Caught exception: {str(e)}, retrying... ({retries+1}/{max_retries})')\n",
    "        retries += 1\n",
    "        time.sleep(5)\n",
    "    print(\"Payload is: \", payload)\n",
    "    raise Exception('Max retries exceeded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:54:34.523298Z",
     "start_time": "2023-07-15T16:54:33.225763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8856440186500549, 0.8737610578536987]\n",
      "0.011882960796356201\n"
     ]
    }
   ],
   "source": [
    "output = query({\n",
    "    \"inputs\": {\n",
    "        \"source_sentence\": text[0],\n",
    "        \"sentences\": [decoded_text[0],decoded_text_q[0]]}\n",
    "        })\n",
    "\n",
    "print(output)\n",
    "print(abs(output[1] - output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:54:49.888454Z",
     "start_time": "2023-07-15T16:54:49.875777Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMLGpD_2P86Z",
    "outputId": "fde59824-dc83-4b33-ea6c-a247f0773c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tokens:  ['en_XX', '▁Patrick', '▁O', \"'\", 'Ne', 'ill', ',', '▁the', '▁former', '▁chief', '▁creative', '▁officer', '▁at', '▁The', 'rano', 's', '▁from', '▁2014', '▁to', '▁2017', ',', '▁said', '▁he', '▁has', '▁removed', '▁much', '▁of', '▁his', '▁promotion', 'al', '▁work', '▁for', '▁the', '▁company', '▁from', '▁his', '▁professional', '▁website', '.']\n",
      "\n",
      "Base Tokens:  ['zh_CN', '▁', '帕', '特', '里', '克', '·', '奥', '尼', '尔', '(', 'Patri', 'ck', '▁O', \"'\", 'Ne', 'ill', ')', '在', '2014', '年', '至', '2017', '年', '间', '担任', 'The', 'rano', 's', '首席', '创意', '总监', ',', '他说', '他', '已经', '从', '自己的', '专业', '网站', '中', '删除', '了', '该公司', '的', '宣传', '工作', '。']\n",
      "\n",
      "Quantized Tokens:  ['zh_CN', '▁', '帕', '特', '里', '克', '·', '奥', '尼', '尔', '(', 'Patri', 'ck', '▁O', \"'\", 'Ne', 'ill', ')', '表示', ',', '他', '已', '从', '自己的', '专业', '网站', '中', '删除', '了', '该公司', '的', '宣传', '工作', '。']\n"
     ]
    }
   ],
   "source": [
    "# convert the input sequence to tokens and exclude special tokens\n",
    "input_tokens = [token for token in tokenizer.convert_ids_to_tokens(encoded_en['input_ids'][0]) if token not in ['<s>', '</s>']]\n",
    "print(\"Input Tokens: \", input_tokens)\n",
    "\n",
    "# convert the output sequence to tokens and exclude special tokens\n",
    "output_tokens = [token for token in tokenizer.convert_ids_to_tokens(generated_tokens[0]) if token not in ['<s>', '</s>']]\n",
    "output_tokens_q = [token for token in tokenizer.convert_ids_to_tokens(generated_tokens_q[0]) if token not in ['<s>', '</s>']]\n",
    "\n",
    "print(\"\\nBase Tokens: \", output_tokens)\n",
    "print(\"\\nQuantized Tokens: \", output_tokens_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:54:56.270377Z",
     "start_time": "2023-07-15T16:54:56.262244Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_tokens(token_list):\n",
    "    processed_list = []\n",
    "    for token in token_list:\n",
    "        if token.startswith('▁'):  # remove the leading underscore\n",
    "            token = token[1:]\n",
    "        processed_list.append(token)\n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:54:56.820335Z",
     "start_time": "2023-07-15T16:54:56.810344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Patrick', 'O', \"'\", 'Ne', 'ill', ',', 'the', 'former', 'chief', 'creative', 'officer', 'at', 'The', 'rano', 's', 'from', '2014', 'to', '2017', ',', 'said', 'he', 'has', 'removed', 'much', 'of', 'his', 'promotion', 'al', 'work', 'for', 'the', 'company', 'from', 'his', 'professional', 'website', '.']\n",
      "['', '帕', '特', '里', '克', '·', '奥', '尼', '尔', '(', 'Patri', 'ck', 'O', \"'\", 'Ne', 'ill', ')', '在', '2014', '年', '至', '2017', '年', '间', '担任', 'The', 'rano', 's', '首席', '创意', '总监', ',', '他说', '他', '已经', '从', '自己的', '专业', '网站', '中', '删除', '了', '该公司', '的', '宣传', '工作', '。']\n",
      "['', '帕', '特', '里', '克', '·', '奥', '尼', '尔', '(', 'Patri', 'ck', 'O', \"'\", 'Ne', 'ill', ')', '表示', ',', '他', '已', '从', '自己的', '专业', '网站', '中', '删除', '了', '该公司', '的', '宣传', '工作', '。']\n"
     ]
    }
   ],
   "source": [
    "input_tokens = process_tokens(input_tokens[1:])\n",
    "output_tokens = process_tokens(output_tokens[1:])\n",
    "output_tokens_q = process_tokens(output_tokens_q[1:])\n",
    "print(input_tokens)\n",
    "print(output_tokens)\n",
    "print(output_tokens_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:55:05.611894Z",
     "start_time": "2023-07-15T16:55:05.605925Z"
    }
   },
   "outputs": [],
   "source": [
    "def compare_tokens(original, quantized):\n",
    "    original_indices = {token: [i for i, x in enumerate(original) if x == token] for token in original}\n",
    "    quantized_indices = {token: [i for i, x in enumerate(quantized) if x == token] for token in quantized}\n",
    "    \n",
    "    additional_tokens = {token: quantized_indices[token] for token in quantized_indices if token not in original_indices}\n",
    "    missing_tokens = {token: original_indices[token] for token in original_indices if token not in quantized_indices}\n",
    "    \n",
    "    return additional_tokens, missing_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:55:06.032023Z",
     "start_time": "2023-07-15T16:55:06.024149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'表示': [17], '已': [20]}\n",
      "{'在': [17], '2014': [18], '年': [19, 22], '至': [20], '2017': [21], '间': [23], '担任': [24], 'The': [25], 'rano': [26], 's': [27], '首席': [28], '创意': [29], '总监': [30], '他说': [32], '已经': [34]}\n"
     ]
    }
   ],
   "source": [
    "addit, miss = compare_tokens(output_tokens, output_tokens_q)\n",
    "print(addit)\n",
    "print(miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:55:16.234990Z",
     "start_time": "2023-07-15T16:55:12.794646Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-07-16 00:55:16,231 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:55:19.976073Z",
     "start_time": "2023-07-15T16:55:18.787318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: (src_index, trg_index) =  [(0, 4), (1, 12), (2, 13), (3, 14), (4, 15), (5, 16), (6, 24), (7, 28), (8, 28), (9, 29), (10, 30), (11, 24), (12, 25), (13, 26), (14, 27), (15, 17), (16, 18), (17, 20), (18, 21), (19, 31), (20, 32), (21, 32), (22, 34), (23, 40), (24, 41), (25, 43), (26, 33), (27, 44), (28, 40), (29, 45), (30, 45), (31, 42), (32, 42), (33, 35), (34, 36), (35, 37), (36, 38), (37, 46)]\n",
      "Quantized: (src_index, trg_index) =  [(0, 4), (1, 12), (2, 13), (3, 14), (4, 15), (5, 9), (6, 1), (7, 25), (8, 23), (9, 30), (10, 24), (11, 22), (12, 28), (13, 10), (13, 11), (14, 11), (15, 17), (16, 8), (17, 5), (18, 18), (19, 16), (20, 17), (21, 19), (22, 20), (23, 26), (24, 27), (25, 29), (26, 22), (27, 30), (28, 26), (29, 31), (30, 31), (31, 28), (32, 28), (33, 21), (34, 22), (35, 23), (36, 24), (37, 32)]\n"
     ]
    }
   ],
   "source": [
    "# The source and target sentences should be tokenized to words.\n",
    "src_sentence = input_tokens\n",
    "trg_sentence = output_tokens\n",
    "trg_sentence_q = output_tokens_q\n",
    "\n",
    "\n",
    "# The output is a dictionary with different matching methods.\n",
    "# Each method has a list of pairs indicating the indexes of aligned words (The alignments are zero-indexed).\n",
    "alignments = myaligner.get_word_aligns(src_sentence, trg_sentence)\n",
    "alignments_q = myaligner.get_word_aligns(src_sentence, trg_sentence_q)\n",
    "\n",
    "src_sent_len = len(src_sentence)\n",
    "base_align = alignments['mwmf']\n",
    "quant_align = alignments_q['mwmf']\n",
    "print(\"Baseline: (src_index, trg_index) = \", base_align)\n",
    "print(\"Quantized: (src_index, trg_index) = \", quant_align)\n",
    "\n",
    "# print(addit)\n",
    "# print(output_tokens_q)\n",
    "# print(miss)\n",
    "# print(output_tokens)\n",
    "# print(src_sentence)\n",
    "# for matching_method in alignments:\n",
    "#     print(matching_method, \":\", alignments[matching_method])\n",
    "\n",
    "# Expected output:\n",
    "# mwmf (Match): [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "# inter (ArgMax): [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]\n",
    "# itermax (IterMax): [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:55:21.545838Z",
     "start_time": "2023-07-15T16:55:21.541529Z"
    }
   },
   "outputs": [],
   "source": [
    "# additional is quantized output token index map; missing is baseline output token index map.\n",
    "def build_hotmap(bmap, qmap, add, miss, src_len):\n",
    "    hotmap = {i: 0 for i in range(src_len)}\n",
    "    for token in add:\n",
    "        for src_index, trg_index in qmap:\n",
    "            if trg_index in add[token]:\n",
    "                hotmap[src_index] += 1\n",
    "                \n",
    "    for token in miss:\n",
    "        for src_index, trg_index in bmap:\n",
    "            if trg_index in miss[token]:\n",
    "                hotmap[src_index] += 1\n",
    "    \n",
    "    return hotmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:55:27.159580Z",
     "start_time": "2023-07-15T16:55:27.151797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 2, 16: 1, 17: 1, 18: 1, 19: 0, 20: 2, 21: 1, 22: 2, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0}\n"
     ]
    }
   ],
   "source": [
    "hot_map = build_hotmap(base_align, quant_align, addit, miss, src_sent_len)\n",
    "print(hot_map)\n",
    "# for key in hot_map:\n",
    "#     if hot_map[key] !=0:\n",
    "#         print(src_sentence[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:56:09.193527Z",
     "start_time": "2023-07-15T16:56:09.180046Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "stop_words = [\n",
    "    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'while', 'as', 'that', 'this',\n",
    "    'these', 'those', 'to', 'for', 'with', 'at', 'from', 'by', 'on', 'off', 'of',\n",
    "    'into', 'over', 'under', 'above', 'below', 'is', 'be', 'am', 'are', 'was',\n",
    "    'were', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'can',\n",
    "    'could', 'shall', 'should', 'will', 'would', 'might', 'must', 'it', 'its',\n",
    "    'it\\'s', 'he', 'his', 'she', 'her', 'hers', 'they', 'their', 'theirs', 'you',\n",
    "    'your', 'yours', 'we', 'our', 'ours', 'in', 'out', 'through', 'because',\n",
    "    'while', 'during', 'before', 'after', 'about', 'against', 'between', 'among',\n",
    "    'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "    'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
    "    'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',\n",
    "    'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',\n",
    "    'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's'\n",
    "]\n",
    "\n",
    "# helper functions\n",
    "def mask_tokens(hit_map, tokens):\n",
    "    non_punctuation_indices = [i for i, token in enumerate(tokens) if token.isalnum()]\n",
    "    weights = [8 if hit_map[i] == 0 else 1/(hit_map[i]+1) for i in non_punctuation_indices]\n",
    "    normalized_weights = [w / sum(weights) for w in weights]\n",
    "\n",
    "    num_unref = sum(1 for i in non_punctuation_indices if hit_map[i] == 0)\n",
    "    num_mask = np.random.randint(max(1, num_unref // 10), 6)\n",
    "    mask_indices = np.random.choice(non_punctuation_indices, size=num_mask, p=normalized_weights, replace=False)\n",
    "\n",
    "    # Continue generating mask_indices until we find one that isn't a stop word\n",
    "    while all(tokens[i] in stop_words for i in mask_indices):\n",
    "        num_mask = np.random.randint(max(1, num_unref // 10), 6)\n",
    "        mask_indices = np.random.choice(non_punctuation_indices, size=num_mask, p=normalized_weights, replace=False)\n",
    "\n",
    "    masked_tokens = tokens.copy()\n",
    "    for i in mask_indices:\n",
    "        if masked_tokens[i] not in stop_words:\n",
    "            masked_tokens[i] = '<fill>'\n",
    "  \n",
    "    return ' '.join(masked_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:56:10.837816Z",
     "start_time": "2023-07-15T16:56:10.823718Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<fill> O ' <fill> ill , the former chief creative officer at The rano s from 2014 to 2017 , said he has removed <fill> of his promotion al work for the company from his professional website .\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked = mask_tokens(hot_map, src_sentence)\n",
    "masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-15T16:56:59.167459Z",
     "start_time": "2023-07-15T16:56:50.339295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxy/anaconda3/lib/python3.9/site-packages/revChatGPT/__init__.py:31: UserWarning: The current Python is not a recommended version, 3.10+ is recommended\n",
      "  __import__(\"warnings\").warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John O'Neill, the former chief creative officer at The Rano's from 2014 to 2017, said he has removed all of his promotional work for the company from his professional website.\n"
     ]
    }
   ],
   "source": [
    "from revChatGPT.V1 import Chatbot\n",
    "\n",
    "chatbot = Chatbot(config={\n",
    "  \"access_token\": \"YOUR ACCESS TOKEN\"})\n",
    "\n",
    "prefix = \"Complete the sentence by filling up the missing information denoted as <fill> in the original sentence: \\n\"\n",
    "prompt = prefix + masked\n",
    "\n",
    "response = \"\"\n",
    "\n",
    "for data in chatbot.ask(prompt):\n",
    "    response = data[\"message\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-14T14:32:39.150912Z",
     "start_time": "2023-07-14T14:32:39.145537Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MzKcMoEzRUdR",
    "outputId": "9ff8a890-7896-4276-8f01-5b69137d7ed4"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# def print_size_of_model(model):\n",
    "#     torch.save(model.state_dict(), \"temp.p\")\n",
    "#     print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "#     os.remove('temp.p')\n",
    "\n",
    "# print_size_of_model(model)\n",
    "# print_size_of_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7T4cM_t7eh-",
    "outputId": "5e49cfca-c86d-45fa-b9ce-dbb080faf460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time 6.116252660751343\n",
      "elapsed time 3.0593185424804688\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# torch.set_num_threads(1)\n",
    "\n",
    "# def time_model_evaluation(model, encoded_data):\n",
    "#     s = time.time()\n",
    "#     model.generate(**encoded_data)\n",
    "#     elapsed = time.time() - s\n",
    "#     print(\"elapsed time\", elapsed)\n",
    "\n",
    "# time_model_evaluation(model, encoded_zh)\n",
    "# time_model_evaluation(quantized_model, encoded_zh)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
